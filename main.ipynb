{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in /opt/anaconda3/lib/python3.12/site-packages (0.10.20)\n",
      "Requirement already satisfied: opencv-python in /opt/anaconda3/lib/python3.12/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: absl-py in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (24.12.23)\n",
      "Requirement already satisfied: jax in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (0.4.38)\n",
      "Requirement already satisfied: jaxlib in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (0.4.38)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (3.9.2)\n",
      "Requirement already satisfied: numpy<2 in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from jax->mediapipe) (0.5.0)\n",
      "Requirement already satisfied: opt_einsum in /opt/anaconda3/lib/python3.12/site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.10 in /opt/anaconda3/lib/python3.12/site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import uuid\n",
    "import os\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "#mp_hands = mp.solutions.hands\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgur.com/qpRACer.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://i.imgur.com/qpRACer.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp_drawing.DrawingSpec(color=(0,0,255), thickness=2, circle_radius=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mp_drawing.draw_landmarks??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import mediapipe as mp\n",
    "\n",
    "# # Initialize Mediapipe FaceMesh and Hands\n",
    "# mp_face_mesh = mp.solutions.face_mesh\n",
    "# mp_hands = mp.solutions.hands\n",
    "\n",
    "# face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1)\n",
    "\n",
    "\n",
    "# # Load the webcam video feed\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# # Define colors for visualizing face mesh and hands\n",
    "# face_colors = {\n",
    "#     'LIPS': (0, 0, 255),  # Red\n",
    "#     'LEFT_EYE': (0, 255, 0),  # Green\n",
    "#     'RIGHT_EYE': (255, 255, 0),  # Cyan\n",
    "#     'FACE_OVAL': (128, 0, 128),  # Purple\n",
    "#     'NOSE': (255, 0, 255),  # Magenta\n",
    "# }\n",
    "\n",
    "# hand_color = (0, 255, 255)  # Yellow\n",
    "\n",
    "# # Loop through webcam frames\n",
    "# while cap.isOpened():\n",
    "#     success, frame = cap.read()\n",
    "#     if not success:\n",
    "#         break\n",
    "\n",
    "#     # Flip and convert the frame for Mediapipe processing\n",
    "#     frame = cv2.flip(frame, 1)\n",
    "#     rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     # Process face mesh and hands\n",
    "#     face_results = face_mesh.process(rgb_frame)\n",
    "#     hand_results = hands.process(rgb_frame)\n",
    "\n",
    "#     # Draw face mesh\n",
    "#     if face_results.multi_face_landmarks:\n",
    "#         for face_landmarks in face_results.multi_face_landmarks:\n",
    "#             for name, connections in [\n",
    "#                 ('LIPS', FACEMESH_LIPS),\n",
    "#                 ('LEFT_EYE', FACEMESH_LEFT_EYE),\n",
    "#                 ('RIGHT_EYE', FACEMESH_RIGHT_EYE),\n",
    "#                 ('FACE_OVAL', FACEMESH_FACE_OVAL),\n",
    "#                 ('NOSE', FACEMESH_NOSE),\n",
    "#             ]:\n",
    "#                 for connection in connections:\n",
    "#                     start_idx, end_idx = connection\n",
    "#                     start_point = face_landmarks.landmark[start_idx]\n",
    "#                     end_point = face_landmarks.landmark[end_idx]\n",
    "\n",
    "#                     # Convert normalized coordinates to pixel coordinates\n",
    "#                     h, w, _ = frame.shape\n",
    "#                     start_coords = (int(start_point.x * w), int(start_point.y * h))\n",
    "#                     end_coords = (int(end_point.x * w), int(end_point.y * h))\n",
    "\n",
    "#                     # Draw the line\n",
    "#                     cv2.line(frame, start_coords, end_coords, face_colors[name], 2)\n",
    "\n",
    "#     # 2. Right hand landmarks\n",
    "#         if results.right_hand_landmarks:\n",
    "#             mp_drawing.draw_landmarks(\n",
    "#                 image,\n",
    "#                 results.right_hand_landmarks,\n",
    "#                 mp_holistic.HAND_CONNECTIONS,\n",
    "#                 mp_drawing.DrawingSpec(color=(80, 22, 10), thickness=2, circle_radius=4),\n",
    "#                 mp_drawing.DrawingSpec(color=(80, 44, 121), thickness=2, circle_radius=2)\n",
    "#             )\n",
    "\n",
    "#             # Extract right hand fingertips\n",
    "#             fingertips = {4: \"Thumb\", 8: \"Index\", 12: \"Middle\", 16: \"Ring\", 20: \"Pinky\"}\n",
    "#             for idx, name in fingertips.items():\n",
    "#                 landmark = results.right_hand_landmarks.landmark[idx]\n",
    "#                 x, y = int(landmark.x * image.shape[1]), int(landmark.y * image.shape[0])\n",
    "#                 cv2.circle(image, (x, y), 5, (0, 255, 0), -1)\n",
    "#                 cv2.putText(image, name, (x + 10, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "#         # 3. Left hand landmarks\n",
    "#         if results.left_hand_landmarks:\n",
    "#             mp_drawing.draw_landmarks(\n",
    "#                 image,\n",
    "#                 results.left_hand_landmarks,\n",
    "#                 mp_holistic.HAND_CONNECTIONS,\n",
    "#                 mp_drawing.DrawingSpec(color=(80, 22, 10), thickness=2, circle_radius=4),\n",
    "#                 mp_drawing.DrawingSpec(color=(80, 44, 121), thickness=2, circle_radius=2)\n",
    "#             )\n",
    "\n",
    "#             # Extract left hand fingertips\n",
    "#             fingertips = {4: \"Thumb\", 8: \"Index\", 12: \"Middle\", 16: \"Ring\", 20: \"Pinky\"}\n",
    "#             for idx, name in fingertips.items():\n",
    "#                 landmark = results.left_hand_landmarks.landmark[idx]\n",
    "#                 x, y = int(landmark.x * image.shape[1]), int(landmark.y * image.shape[0])\n",
    "#                 cv2.circle(image, (x, y), 5, (0, 255, 0), -1)\n",
    "#                 cv2.putText(image, name, (x + 10, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "#     # Show the frame\n",
    "#     cv2.imshow('Hands and Face Mesh', frame)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # Release resources\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Function to calculate Euclidean distance\n",
    "def calculate_distance(point1, point2):\n",
    "    return math.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1735649369.505766 2794037 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M2\n",
      "W0000 00:00:1735649369.512144 2808543 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1735649369.512653 2794037 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M2\n",
      "W0000 00:00:1735649369.528117 2808543 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1735649369.539373 2808547 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1735649369.555697 2808547 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1735649369.632173 2794037 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M2\n",
      "W0000 00:00:1735649369.720376 2808597 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1735649369.733058 2808597 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1735649369.736920 2808596 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1735649369.737170 2808597 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1735649369.738355 2808598 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1735649369.746672 2808596 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1735649369.760436 2808602 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1735649369.762434 2808598 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mflip(image, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Make Detections\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m results \u001b[38;5;241m=\u001b[39m holistic\u001b[38;5;241m.\u001b[39mprocess(image)\n\u001b[1;32m     84\u001b[0m hand_results \u001b[38;5;241m=\u001b[39m hands\u001b[38;5;241m.\u001b[39mprocess(image)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Recolor image back to BGR for rendering\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/mediapipe/python/solutions/holistic.py:160\u001b[0m, in \u001b[0;36mHolistic.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[1;32m    137\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks, left and right hand landmarks, and face landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mprocess(input_data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: image})\n\u001b[1;32m    161\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/mediapipe/python/solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[1;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mwait_until_idle()\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Holistic, FaceMesh, Drawing utilities\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Define MediaPipe FaceMesh connections\n",
    "FACEMESH_LIPS = frozenset([(61, 146), (146, 91), (91, 181), (181, 84), (84, 17),\n",
    "                           (17, 314), (314, 405), (405, 321), (321, 375),\n",
    "                           (375, 291), (61, 185), (185, 40), (40, 39), (39, 37),\n",
    "                           (37, 0), (0, 267),\n",
    "                           (267, 269), (269, 270), (270, 409), (409, 291),\n",
    "                           (78, 95), (95, 88), (88, 178), (178, 87), (87, 14),\n",
    "                           (14, 317), (317, 402), (402, 318), (318, 324),\n",
    "                           (324, 308), (78, 191), (191, 80), (80, 81), (81, 82),\n",
    "                           (82, 13), (13, 312), (312, 311), (311, 310),\n",
    "                           (310, 415), (415, 308)])\n",
    "\n",
    "FACEMESH_LEFT_EYE = frozenset([(263, 249), (249, 390), (390, 373), (373, 374),\n",
    "                               (374, 380), (380, 381), (381, 382), (382, 362),\n",
    "                               (263, 466), (466, 388), (388, 387), (387, 386),\n",
    "                               (386, 385), (385, 384), (384, 398), (398, 362)])\n",
    "\n",
    "FACEMESH_RIGHT_EYE = frozenset([(33, 7), (7, 163), (163, 144), (144, 145),\n",
    "                                (145, 153), (153, 154), (154, 155), (155, 133),\n",
    "                                (33, 246), (246, 161), (161, 160), (160, 159),\n",
    "                                (159, 158), (158, 157), (157, 173), (173, 133)])\n",
    "\n",
    "FACEMESH_FACE_OVAL = frozenset([(10, 338), (338, 297), (297, 332), (332, 284),\n",
    "                                (284, 251), (251, 389), (389, 356), (356, 454),\n",
    "                                (454, 323), (323, 361), (361, 288), (288, 397),\n",
    "                                (397, 365), (365, 379), (379, 378), (378, 400),\n",
    "                                (400, 377), (377, 152), (152, 148), (148, 176),\n",
    "                                (176, 149), (149, 150), (150, 136), (136, 172),\n",
    "                                (172, 58), (58, 132), (132, 93), (93, 234),\n",
    "                                (234, 127), (127, 162), (162, 21), (21, 54),\n",
    "                                (54, 103), (103, 67), (67, 109), (109, 10)])\n",
    "\n",
    "FACEMESH_NOSE = frozenset([(168, 6), (6, 197), (197, 195), (195, 5),\n",
    "                           (5, 4), (4, 1), (1, 19), (19, 94), (94, 2), (98, 97),\n",
    "                           (97, 2), (2, 326), (326, 327), (327, 294),\n",
    "                           (294, 278), (278, 344), (344, 440), (440, 275),\n",
    "                           (275, 4), (4, 45), (45, 220), (220, 115), (115, 48),\n",
    "                           (48, 64), (64, 98)])\n",
    "\n",
    "# Threshold for fingertip proximity to facial features\n",
    "THRESHOLD_DISTANCE = 25 #(in pixels)\n",
    "\n",
    "\n",
    "# Colors for different face features\n",
    "face_colors = {\n",
    "    'LIPS': (0, 0, 255),  # Red\n",
    "    'LEFT_EYE': (255, 255, 0),  # Cyan\n",
    "    'RIGHT_EYE': (255, 255, 0),  # Cyan\n",
    "    'FACE_OVAL': (128, 0, 128),  # Purple\n",
    "    'NOSE': (0, 255, 0),  # Green\n",
    "}\n",
    "\n",
    "# Initialize MediaPipe FaceMesh and Hands\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1)\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to access the camera.\")\n",
    "    exit()\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Unable to read from the camera.\")\n",
    "            break\n",
    "\n",
    "        # Recolor Feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # Flip horizontal\n",
    "        image = cv2.flip(image, 1)\n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        hand_results = hands.process(image)\n",
    "\n",
    "        # Recolor image back to BGR for rendering\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Draw custom face mesh\n",
    "        if results.face_landmarks:\n",
    "            for name, connections in [\n",
    "                ('LIPS', FACEMESH_LIPS),\n",
    "                ('LEFT_EYE', FACEMESH_LEFT_EYE), #SWAPPED DUE TO FLIPPED CAMERA\n",
    "                ('RIGHT_EYE', FACEMESH_RIGHT_EYE),\n",
    "                ('FACE_OVAL', FACEMESH_FACE_OVAL),\n",
    "                ('NOSE', FACEMESH_NOSE),\n",
    "            ]:\n",
    "                for connection in connections:\n",
    "                    start_idx, end_idx = connection\n",
    "                    start_point = results.face_landmarks.landmark[start_idx]\n",
    "                    end_point = results.face_landmarks.landmark[end_idx]\n",
    "\n",
    "                    # Convert normalized coordinates to pixel coordinates\n",
    "                    h, w, _ = frame.shape\n",
    "                    start_coords = (int(start_point.x * w), int(start_point.y * h))\n",
    "                    end_coords = (int(end_point.x * w), int(end_point.y * h))\n",
    "\n",
    "                    # Draw the line with custom color\n",
    "                    cv2.line(image, start_coords, end_coords, face_colors[name], 2)\n",
    "\n",
    "        # Check for right hand landmarks\n",
    "        if hand_results.multi_hand_landmarks:\n",
    "            for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "                # Draw hand landmarks\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    hand_landmarks,\n",
    "                    mp_holistic.HAND_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(80, 22, 10), thickness=2, circle_radius=4),\n",
    "                    mp_drawing.DrawingSpec(color=(80, 44, 121), thickness=2, circle_radius=2)\n",
    "                )\n",
    "\n",
    "                # Extract hand fingertips\n",
    "                fingertips = {4: \"Thumb\", 8: \"Index\", 12: \"Middle\", 16: \"Ring\", 20: \"Pinky\"}\n",
    "                for idx, name in fingertips.items():\n",
    "                    landmark = hand_landmarks.landmark[idx]\n",
    "                    x, y = int(landmark.x * image.shape[1]), int(landmark.y * image.shape[0])\n",
    "                    cv2.circle(image, (x, y), 5, (0, 255, 0), -1)\n",
    "                    cv2.putText(image, name, (x + 10, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "                \n",
    "                    # Check for proximity to facial features\n",
    "                    if results.face_landmarks:\n",
    "                        for name, connections in [\n",
    "                            ('LEFT_EYE', FACEMESH_LEFT_EYE),\n",
    "                            ('RIGHT_EYE', FACEMESH_RIGHT_EYE),\n",
    "                            ('LIPS', FACEMESH_LIPS),\n",
    "                            ('NOSE', FACEMESH_NOSE),\n",
    "                            ('FACE_OVAL', FACEMESH_FACE_OVAL)\n",
    "                        ]:\n",
    "                            for connection in connections:\n",
    "                                start_idx, end_idx = connection\n",
    "                                start_point = results.face_landmarks.landmark[start_idx]\n",
    "                                end_point = results.face_landmarks.landmark[end_idx]\n",
    "                \n",
    "                                # Convert to pixel coordinates\n",
    "                                h, w, _ = frame.shape\n",
    "                                start_coords = (int(start_point.x * w), int(start_point.y * h))\n",
    "                                end_coords = (int(end_point.x * w), int(end_point.y * h))\n",
    "                \n",
    "                                # Calculate the distance to the fingertip\n",
    "                                fingertip_coords = (x, y)\n",
    "                                distance_to_start = calculate_distance(fingertip_coords, start_coords)\n",
    "                                distance_to_end = calculate_distance(fingertip_coords, end_coords)\n",
    "                \n",
    "                                # If the fingertip is close to the face feature, show an alert\n",
    "                                if distance_to_start < THRESHOLD_DISTANCE or distance_to_end < THRESHOLD_DISTANCE:\n",
    "                                    #alert_count += 1\n",
    "                                    if name == \"LIPS\":\n",
    "                                        cv2.putText(image, 'Biting nails', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                                    elif name == \"LEFT_EYE\":\n",
    "                                        cv2.putText(image, 'Rubbing Right Eye', (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "                                    elif name == \"RIGHT_EYE\":\n",
    "                                        cv2.putText(image, 'Rubbing Left Eye', (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "                                    elif name == \"NOSE\":\n",
    "                                        cv2.putText(image, 'Booger Alert!', (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                                    elif name == \"FACE_OVAL\":\n",
    "                                        cv2.putText(image, 'Potential skin picking', (50, 200), cv2.FONT_HERSHEY_SIMPLEX, 1, (128, 0, 128), 2, cv2.LINE_AA)\n",
    "\n",
    "                                # After checking all features, check if multiple alerts were detected\n",
    "                                # if alert_count > 1:\n",
    "                                #     cv2.putText(image, 'MULTIPLE ALERTS DETECTED', (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv3.12)",
   "language": "python",
   "name": "venv3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
